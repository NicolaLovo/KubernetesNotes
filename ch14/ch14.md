# Chapter 14 - Taints and Tolerations

Ex. I have 3 nodes: node A,B,C

- node A has GPUs -> I assign the taint gpu=true
  - allow only pods with toleration gpu=true to be scheduled on it

- pod1 -> has no toleration, goes to B
- pod2 has toleration gpu=true -> enables it to be scheduled on the node with the corresponding toleration

Use cases:

- have nodes with specialized workloads

## Taints

**Key point: a node reject pods**

Go on nodes.

Specify: label, value, effect.

Can have one of 3 scheduling effects:

- NoExecute -> applied to *existing and new* pods
  - existing pods that do no match are evicted
- NoSchedule -> a *new pod* without such toleration cannot be scheduled on it
  - current pods are not moved
- PreferNoSchedule -> best effort version of noSchedule

## Tolerations

Go on pods

Important: the effect needs to be specified also on the pod toleration and must be equal to the node taint, since it means: *the pod is willing to accept a toleration with this key, value and effect*

- if effects do not match, the pod won't accept the taint

Kubernetes adds automatically 2 tolerations to each pod:

- node.kubernetes.io/not-ready
  - taint applied to a node when it is known to control plane, but is unhealthy (es. kubelet not responding, disk, memory, or network issues on the node)
  - this toleration allows the pod to remain temporarily on this unhealthy node instead of being immediately eviceted
- node.kubernetes.io/unreachable
  - taint applied when the node becomes completely unreachable (es. node crash or power loss)
  - prevents immediate eviction of Pods when connectivity is briefly lost

Without these, a brief node issue could cause immediate storms of pod evictions -> stability mechanism

### Demo

- `kubectl taint node myNode key=value:effect` -> taint a node
  - es. `kubectl taint node node-1 gpu=true:NoSchedule`
- `kubectl run nginx --image=nginx` -> no toleration
  - if there is just one node and has a taint, the pod will be stuck in pending state
- `kubectl run nginx --image=nginx --dry-run=client -o yaml > pod.yaml` 

## Node selectors

**Key point: a pod chooses a node**

There is a problem: a node with a toleration may be scheduled on a node without the corresponding taint, there is no guarantee. Taints are useful to tell "do not scheduel pods on such node, unless they tolerate it"

Solution: `nodeSelector`

Give the decision to pods -> they select the node where to be scheduled

- if pod has label gpu=true, it will go ONLY on nodes with gpu=true
- it is a hard requirement, not a preference

Add a label to a node:

- `kubectl label node myNode gpu=false`


| Mechanism    | Who controls it      | Default behavior    |
| ------------ | -------------------- | ------------------- |
| nodeSelector | Pod author           | Pod can go anywhere |
| Taints       | Cluster / node admin | Pod is rejected     |
