# Chapter 32 - Kubernetes Networking

10 years before, Docker was bundled in Kubernetes since it was the only container runtime available

Now we have the OCI (Open Container Initiative), has 3 specs:

- image spec
- runtime spec
- distribution spec

From Kubernetes 1.24, it supports OCI.

The modern Docker architecture is:

```
Docker run 
-> daemon (does not start the container)
-> containerd (does not start the container): controls the lifecycle of the container (es. pull the image)
-> makes the request to the Shim
-> shim calls runc: uses the OCI runtime to start the container
```

So on a kubernetes node we have the kubelet and the CRI (Container Runtime Interface):

- to create a container the kubelet calls the CRI
- the CRI passes the request to the appropriate container runtime (e.g. containerd)
- then CNI (Container Network Interface) configures the network of the container

## CNI

- specification that defines how networking works between containers -> there are many implementations (e.g. flannel, calico, cilium...)
  - cilium -> de facto standard
- in Kubernetes every pod can communicate with any other Pod -> every pod requires an IP
  - the IP is provided by the CNI

Multi-container pod communication

- the CNI attaches the IP address to the pod's `eth0` interface (the interface name is decided by the CNI plugin)
  - communicates with the `veth` interface of the host
- the pod creates the container specified plus a container named `pause` -> holds the network namespace together
  - it is thanks to the pause container that all containers in the same pod can communicate in localhost

Inter-node pod communication: A on node 1, B on node 2

- A communicates to veth1 -> veth2 -> B
- veth1 acts like a tunnel and traffic goes to the root Network Namespace
  - the bridge resolves the destination address using the ARP table
  - veth2 sends traffic to B
